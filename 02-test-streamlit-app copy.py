# ì±— íˆìŠ¤í† ë¦¬ + ê° ë‹µë³€ë°›ê¸° + ë§í’ì„  êµ¬í˜„

# pip install -r requirements.txt
# streamlit run 02-test-streamlit-app.py

import os
import streamlit as st
from dotenv import load_dotenv
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI

from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import OpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import speech_recognition as sr
from gtts import gTTS
from io import BytesIO
import base64

# ë”ë¯¸ ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±
dummy_data = [
    {"question": "ë”ë¯¸ ì§ˆë¬¸ 1: ê¹€ì¹˜ì°Œê°œ ìš”ë¦¬ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.", "answer": "ë”ë¯¸ ì‘ë‹µ 1: ê¹€ì¹˜ì°Œê°œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ë¼ì§€ê³ ê¸°ì™€ ê¹€ì¹˜ë¥¼ ë³¶ì•„ì•¼ í•©ë‹ˆë‹¤."},
    {"question": "ë”ë¯¸ ì§ˆë¬¸ 2: ëœì¥ì°Œê°œëŠ” ì–´ë–»ê²Œ ë§Œë“¤ì£ ?", "answer": "ë”ë¯¸ ì‘ë‹µ 2: ëœì¥ì°Œê°œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ëœì¥ê³¼ ê³ ì¶”ì¥ì„ í’€ê³  ì•¼ì±„ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤."},
    {"question": "ë”ë¯¸ ì§ˆë¬¸ 3: ê³„ë€ë§ì´ëŠ” ì–´ë–»ê²Œ ë§Œë“œë‚˜ìš”?", "answer": "ë”ë¯¸ ì‘ë‹µ 3: ê³„ë€ë§ì´ëŠ” ê³„ë€ì„ í’€ì–´ì„œ ì˜ ìµíŒ í›„ ëŒëŒ ë§ì•„ ë§Œë“­ë‹ˆë‹¤."},
    {"question": "ë”ë¯¸ ì§ˆë¬¸ 4: ì¡ì±„ ë§Œë“œëŠ” ë²•ì€?", "answer": "ë”ë¯¸ ì‘ë‹µ 4: ì¡ì±„ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë‹¹ë©´ì„ ì‚¶ê³  ë‹¤ì–‘í•œ ì•¼ì±„ë¥¼ ë³¶ì•„ì•¼ í•©ë‹ˆë‹¤."},
    {"question": "ë”ë¯¸ ì§ˆë¬¸ 5: ê°ˆë¹„ì°œ ë§Œë“œëŠ” ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.", "answer": "ë”ë¯¸ ì‘ë‹µ 5: ê°ˆë¹„ì°œì€ ê°ˆë¹„ë¥¼ ì‚¶ê³  ì–‘ë…ì„ ë„£ì–´ í‘¹ ë“ì—¬ì•¼ ë§›ìˆìŠµë‹ˆë‹¤."}
]

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="pAIk ì£¼ë¶€ ìš”ë¦¬ ë¹„ì„œ", layout="centered", initial_sidebar_state="collapsed")

# Streamlit UIë¥¼ ì»¤ìŠ¤í…€í•˜ê¸° ìœ„í•´ HTMLê³¼ CSS ì‚¬ìš©
st.markdown(
     """
    <style>
    /* í˜ì´ì§€ ì „ì²´ ë°°ê²½ ìƒ‰ìƒ */
    .main {
        background-color: black;
    }
    
    /* í…ìŠ¤íŠ¸ ìƒ‰ìƒ */
    .stTextInput, .stButton, .stAlert, .stMarkdown {
        color: white !important;
    }

    /* ë²„íŠ¼ ìƒ‰ìƒ ë³€ê²½ */
    .stButton button, .disabled-button {
        background-color: #444;
        color: white;
        border-radius: 5px;
        height: 50px;  /* ë²„íŠ¼ ë†’ì´ ì„¤ì • */
        font-size: 16px;
    }

    /* ë¹„í™œì„±í™”ëœ ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .disabled-button {
        background-color: #777;  
        color: white;
        border-radius: 5px;
        opacity: 0.6;
        pointer-events: none;
    }

    /* í…ìŠ¤íŠ¸ ì…ë ¥ ìƒì ë°°ê²½ ë° í…ìŠ¤íŠ¸ ìƒ‰ìƒ */
    .stTextInput input {
        background-color: #333;
        color: white;
        height: 50px;
    }

    /* í˜ì´ì§€ ì œëª© ìƒ‰ìƒ ë³€ê²½ */
    h1 {
        color: #FFD700 !important; 
    }

    /* ë§í’ì„  ìŠ¤íƒ€ì¼ */
    .chat-bubble {
        max-width: 60%;
        padding: 10px;
        border-radius: 15px;
        margin-bottom: 10px;
        word-wrap: break-word;
        position: relative;
        display: inline-block;
    }

    /* ì‚¬ìš©ì ë§í’ì„  */
    .user-bubble {
        background-color: #0084ff;
        color: white;
        align-self: flex-end;
    }
    
    /* ì‚¬ìš©ì ë§í’ì„  ê¼¬ë¦¬ */
    .user-bubble::after {
        content: "";
        position: absolute;
        bottom: 0;
        right: -10px;
        width: 0;
        height: 0;
        border-left: 10px solid #0084ff;
        border-top: 10px solid transparent;
        border-bottom: 10px solid transparent;
    }

    /* AI ë§í’ì„  */
    .assistant-bubble {
        background-color: #e4e6eb;
        color: black;
        align-self: flex-start;
    }

    /* AI ë§í’ì„  ê¼¬ë¦¬ */
    .assistant-bubble::after {
        content: "";
        position: absolute;
        bottom: 0;
        left: -10px;
        width: 0;
        height: 0;
        border-right: 10px solid #e4e6eb;
        border-top: 10px solid transparent;
        border-bottom: 10px solid transparent;
    }

    /* ì±„íŒ… ì˜ì—­ ë ˆì´ì•„ì›ƒ ì„¤ì • */
    .chat-container {
        display: flex;
        flex-direction: column;
        margin: 20px 0;
    }

    /* ìŠ¤í¬ë¡¤ë°” ìƒ‰ìƒ */
    ::-webkit-scrollbar {
        width: 10px;
    }
    ::-webkit-scrollbar-track {
        background: #333;
    }
    ::-webkit-scrollbar-thumb {
        background: #888;
    }
    ::-webkit-scrollbar-thumb:hover {
        background: #555;
    }
    </style>
    """, 
    unsafe_allow_html=True
)

# ì œëª©ì„ HTMLë¡œ ì»¤ìŠ¤í…€í•˜ì—¬ í‘œì‹œ
st.markdown("<h1>pAIk ì£¼ë¶€ ìš”ë¦¬ ë¹„ì„œ</h1>", unsafe_allow_html=True)

# ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•  ë•Œ ì´ ì‹¤í–‰íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ì— .env ë¼ëŠ” ì´ë¦„ì˜ íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ChatGPT ì‚¬ìš©ì„ ìœ„í•œ API KEYì™€ ê°™ì€
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
# .env íŒŒì¼ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ì´ ë¯¸ë¦¬ ì •ì˜í•´ ë‘ì–´ì•¼ í•©ë‹ˆë‹¤. 
# OPENAI_API_KEY="sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
#
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ë¡œë“œ
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()  # API í‚¤ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ í•¨

# api_key = os.getenv("OPENAI_API_KEY")
# if not api_key:
#     st.error("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
# else:
#     st.write("OpenAI API Keyê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


# FAISS ë²¡í„° ìŠ¤í† ì–´ëŠ” 24-vectorstore-save.ipynbì—ì„œ ì €ì¥í•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ì´ íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ìˆëŠ” ë””ë ‰í† ë¦¬ í•˜ìœ„ì— db/faiss ë””ë ‰í† ë¦¬ì— ë²¡í„° ìŠ¤í† ì–´ DB íŒŒì¼ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤

# FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ ìˆ˜ì •
def load_faiss():
    # ë¡œë”© ì¤‘ ë©”ì‹œì§€ë¥¼ ìœ„í•œ placeholder ìƒì„±
    loading_message = st.empty()
    
    # ë¡œë”© ì¤‘ ë©”ì‹œì§€ í‘œì‹œ
    loading_message.markdown('<div style="text-align: center; font-size: 18px; color: #FFD700;">AI ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš” ğŸ˜Š</div>', unsafe_allow_html=True)
    
    # ì‹¤ì œ FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vector_store = FAISS.load_local('./db/faiss', OpenAIEmbeddings(), allow_dangerous_deserialization=True)

    # ë¡œë“œ ì™„ë£Œ í›„ ê¸°ì¡´ ë¡œë”© ë©”ì‹œì§€ë¥¼ ì„±ê³µ ë©”ì‹œì§€ë¡œ ë³€ê²½
    loading_message.markdown('<div style="text-align: center; font-size: 18px; color: #FFD700;">AI ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ¤–ğŸ³</div>', unsafe_allow_html=True)
    
    return vector_store



#ê¸°ì¡´ ì½”ë“œ
# def load_faiss():
#     st.write("FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")    
#     embeddings = OpenAIEmbeddings(openai_api_key=api_key)
#     vector_store = FAISS.load_local('./db/faiss', OpenAIEmbeddings(), allow_dangerous_deserialization=True)
#     st.write("ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
#     return vector_store

# Google Speech Recognitionì„ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ í•¨ìˆ˜
def recognize_speech():
    r = sr.Recognizer()
    
    # ë§ˆì´í¬ë¡œë¶€í„° ìŒì„± ì…ë ¥ ë°›ê¸°
    with sr.Microphone() as source:
        st.write("ìŒì„± ì¸ì‹ì¤‘.....")
        r.adjust_for_ambient_noise(source)  # ì£¼ë³€ ì†ŒìŒ ì¡°ì ˆ
        audio = r.listen(source)  # ìŒì„± ì…ë ¥ ë°›ê¸°
    
    try:
        # Google Speech Recognitionì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = r.recognize_google(audio, language='ko-KR')
        st.write("ì¸ì‹ ì™„ë£Œ")
        return text
    except sr.UnknownValueError:
        st.write("ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    except sr.RequestError as e:
        st.write(f"Google Speech Recognition ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¬ìƒí•˜ëŠ” í•¨ìˆ˜ (gTTS ì‚¬ìš©)
def speak_text_gtts(text):
    tts = gTTS(text=text, lang='ko')
    mp3_fp = BytesIO()
    tts.write_to_fp(mp3_fp)
    mp3_fp.seek(0)

    # ìŒì„±ì„ ë¸Œë¼ìš°ì €ì—ì„œ ì¬ìƒí•˜ê¸° ìœ„í•´ base64ë¡œ ì¸ì½”ë”©
    audio_data = base64.b64encode(mp3_fp.read()).decode("utf-8")
    audio_html = f'<audio autoplay="true" controls><source src="data:audio/mp3;base64,{audio_data}" type="audio/mp3"></audio>'
    
    st.markdown(audio_html, unsafe_allow_html=True)

# st.session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ë”ë¯¸ ë°ì´í„°ì˜ í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ì €ì¥
if "dummy_index" not in st.session_state:
    st.session_state["dummy_index"] = 0

# ì±„íŒ… ë©”ì‹œì§€ë¥¼ session_stateì— ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ êµ¬í˜„
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ë§ (ë§í’ì„  ëª¨ì–‘ ì ìš©)
def display_chat_message(role, message, index=None):
    if role == "user":
        st.markdown(f"""
            <div class="chat-bubble user-bubble">
                ğŸ§‘ {message}
            </div>
        """, unsafe_allow_html=True)
    elif role == "assistant":
        st.markdown(f"""
            <div class="chat-bubble assistant-bubble">
                ğŸ¥£ {message}
            </div>
        """, unsafe_allow_html=True)
        # ì‘ë‹µ ìŒì„± ë“£ê¸° ë²„íŠ¼ì— ê³ ìœ  í‚¤ ì¶”ê°€ (ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸¸ì´ì™€ indexë¥¼ í•¨ê»˜ ì‚¬ìš©)
        if index is not None:
            unique_key = f"listen_button_{index}_{len(st.session_state['chat_history'])}"
            if st.button(f"ğŸ”Š", key=unique_key):
                speak_text_gtts(message)  


# ì±—ë´‡ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥ í•¨ìˆ˜
def display_chat_history():
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    for i, entry in enumerate(st.session_state["chat_history"]):
        display_chat_message(entry["role"], entry["message"], index=i if entry["role"] == "assistant" else None)
    st.markdown('</div>', unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜
def process_user_input(query, vector_store):
    if not query:
        st.markdown('<div class="warning-text">ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.</div>', unsafe_allow_html=True)
        return
    
    st.markdown('<div class="full-width-text">ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...</div>', unsafe_allow_html=True)
        
    from langchain_core.prompts import PromptTemplate
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. 
        í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

#Question:
{question}

#Context:
{context}

#Answer:"""
    )
    

    ###############
    # ë‹µë³€ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ ì‚¬ìš©í•œ openapi
    from langchain.chat_models import ChatOpenAI  # OpenAI APIë¥¼ ì‚¬ìš©í•œ LLM

    llm = ChatOpenAI(openai_api_key=api_key, temperature=0.5)  # OpenAI APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë¶€ë¶„
    retriever = vector_store.as_retriever()
    rag_chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
    result = rag_chain.invoke(query)
    ################


    # from langchain_community.chat_models import ChatOllama
    # from langchain_core.output_parsers import StrOutputParser
    # from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
    # from langchain_core.callbacks.manager import CallbackManager

    # llm = ChatOllama(model="llama2:7b", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
    # retriever = vector_store.as_retriever()
    # rag_chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
    # result = rag_chain.invoke(query)

    # ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ì‚¬ìš©ì ì…ë ¥ ë° AI ì‘ë‹µ)
    st.session_state["chat_history"].append({"role": "user", "message": query})
    st.session_state["chat_history"].append({"role": "assistant", "message": result})

    # ì‚¬ìš©ì ë° AI ì‘ë‹µ ë©”ì‹œì§€ í‘œì‹œ
    display_chat_history()

# ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€í™” í…ŒìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
def process_dummy_data():
    # í˜„ì¬ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë”ë¯¸ ì§ˆë¬¸ê³¼ ì‘ë‹µì„ ê°€ì ¸ì˜´
    if st.session_state["dummy_index"] < len(dummy_data):
        data = dummy_data[st.session_state["dummy_index"]]
        # ì¤‘ë³µëœ ì§ˆë¬¸ì´ë‚˜ ì‘ë‹µì´ ê¸°ë¡ë˜ì§€ ì•Šë„ë¡ í™•ì¸
        if not any(entry['message'] == data['question'] for entry in st.session_state["chat_history"]):
            st.session_state["chat_history"].append({"role": "user", "message": data["question"]})
        if not any(entry['message'] == data['answer'] for entry in st.session_state["chat_history"]):
            st.session_state["chat_history"].append({"role": "assistant", "message": data["answer"]})
        # ì¸ë±ìŠ¤ë¥¼ ì¦ê°€ì‹œì¼œ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ í•¨
        st.session_state["dummy_index"] += 1
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    display_chat_history()


# ì±—ë´‡ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
display_chat_history()

# ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
vector_store = load_faiss()

st.write("ì €ëŠ” ìš”ë¦¬ë¥¼ ë„ì™€ ë“œë¦¬ëŠ” ìš”ë¦¬ì‚¬ ë¹„ì„œì…ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.text_input("ê¶ê¸ˆí•œ ìš”ë¦¬ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.")

# ì§ˆë¬¸ ì²˜ë¦¬ ë²„íŠ¼
if st.button("ì§ˆë¬¸ í•˜ê¸°"):
    process_user_input(user_input, vector_store)

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ë²„íŠ¼
if st.button("ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸°"):
    process_dummy_data()